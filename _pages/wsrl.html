---
permalink: /wsrl/
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Tiered Reward: Designing Rewards for Specification and Fast Learning of Desired Behavior">
  <meta name="keywords" content="Principaled Reward Design, Fast Learning, Reinfrocement Learning, Pareto Optimality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Tiered Reward</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
  <link rel="icon" href="https://example.com/path/to/favicon.ico">
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Efficient Online Reinforcement Learning
            Fine-Tuning Need Not Retain Offline Data</h1>
          <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhouzypaul.github.io/">Zhiyuan Zhou</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href=""></a>Andy Peng<sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href=""></a>Qiyang Li<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Sergey Levine</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Aviral Kumar</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup><font size="-0.1">1</sup>UC Berkeley</font></span>
            <span class="author-block"><sup><font size="-0.1">2</sup>Carnegie Mellon University </font></span>
            <span class="author-block"><sup><font size="-0.1">*</sup>Equal Contribution </font></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.20635"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/rail-berkeley/soar"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/rail-berkeley/soar?tab=readme-ov-file#using-soar-data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="
                      far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="video-background">
  <div class="container is-max-desktop has-text-centered">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline width="90%">
        <source src="static/videos/teaser_video.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="../images/paper-images/wsrl_teaser.pdf" alt="System Diagram" width="100%">
      <h2 class="subtitle has-text-centered">
        No data retention fine-tuning focuses on RL fine-tuning without using the offline dataset during online updates, mirroring the common paradigm in machine learning at scale today. The offline dataset is only used to pre-train a policy and Q-function via offline RL to initialize fine-tuning, after which the dataset is discarded and the agent only fine-tunes with online experience. Current methods struggle in this no-retention setting and forget knowledge learned from pre-training. Our goal is to develop a fine-tuning method that quickly adapts online even if we do not retain offline data.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <div class="custom-box">
      <h2 class="title is-2">TL;DR</h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              We investigate the role of offline data retention during online RL fine-tuning and the sudden divergence in value functions that arises due to the distribution mismatch 
              between the offline dataset and online rollouts. This divergence typically results in unlearning and forgetting the benefits of offline pre-training. To address this problem, we introduce Warm-Start RL (WSRL), 
              an Actor-Critic off-policy algorithm that performs efficient offline-to-online fine-tuning without retaining offline data.
              By initializing the value function and policy with a pretrained Q-function and policy and using warmup steps to collect online rollouts with the frozen offline 
              policy, WSRL is able to simulate the retention of offline data and mitigate the challenges of distribution shift and catestrophic forgetting that comes with 
              offline-to-online RL finetuning.
            </p>
          </div>
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Motivation</h2>
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), 
            this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. 
            Such a setting is important for truly scalable RL.
    
          </p>
          <p>
            However, continued training on offline data for stability and performance can be slow, expensive, and limiting due to constraints or pessimism on the offline data. 
            We find that previous offline-to-online RL algorithms fail completely in this setting because of Q-value divergence due to distribution shift. 
            Is it possible to fine-tune from offline RL value and policy initializations, but without retaining offline data or forgetting the pre-training? 
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">What are the challenges?</h2>
    <div class="columns">
      <div class="column">

        <div class="custom-box" style="padding-bottom: 20px; margin-bottom: 20px;">
          <div class="content has-text-justified">
            <h3> 
              Distribution shift between offline and online data destroys Q-function fit
            </h3>
            <p>
              Training only on online experience during fine-tuning without offline data retention can destroy how well the model fits offline data: despite attaining comparable TD-errors on the online data to the setting when offline data is retained, TD-errors under the offline distribution grow larger
            </p>
          </div>
        </div>

        <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
          <img src="../images/paper-images/calql_different_mixing.png" style="width: 25%;" alt="Mixing Image">
          <img src="../images/paper-images/calql_different_mixing_q_blowup.png" style="width: 25%;" alt="Q values Blowup">
          <img src="../images/paper-images/calql_different_mixing_loss_blowup.png" style="width: 25%;" alt="TD Error Blowup">
          <img src="../images/paper-images/calql_different_mixing_loss_online.png" style="width: 25%;" alt="TD Loss Online">
        </div>
        

        <div class="content has-text-justified">
          <div class="custom-box" style="padding-bottom: 20px; margin-bottom: 20px;">
            <h3> 
              Re-calibration of Q-values leads to excessive underestimation
            </h3>
            <p>
              The distribution shift between offline dataset data and online rollout data distributions causes a divergence of Q values at the onset of finetuning. We find that Q-value recalibration at the beginning of fine-tuning leads to excessive underestimation due to backups with over-pessimistic TD-targets. We call this the Downward Spiral, and is shown across 3 kitchen envs with 3 offline RL methods
            </p>
          </div>
        </div>

        <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
          <img src="../images/paper-images/downwards_spiral_cql.png" style="width: 33%;" alt="Downward Spiral CQL">
          <img src="../images/paper-images/downward-spiral-calql.png" style="width: 33%;" alt="Downward Spiral CalQL">
          <img src="../images/paper-images/downward-spiral-iql.png" style="width: 33%;" alt="Downward Spiral IQL">
        </div>

        <div class="content has-text-justified">
          <div class="custom-box" style="padding-bottom: 20px; margin-bottom: 20px;">
            <h3> 
              Retaining offline data hurts asymptotic performance
            </h3>
            <p>
              While retaining offline data appears to be crucial for preventing forgetting at the beginning of fine-tuning for current fine-tuning methods,
               continuing to make updates on this offline data with an (pesimistic) offline RL algorithm negatively impacts asymptotic performance and efficiency.
               RLPD starts from scratch, and is able to outperform CalQL with data retention.
            </p>
          </div>
          <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
            <img id="teaser" src="../images/paper-images/calql_rlpd_compare.png" alt="System Diagram" width="100%">
          </div>


    </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
      <video id="teaser" autoplay muted playsinline height="100%">
        <source src="./static/videos/animated_diagram.mp4"
                type="video/mp4">
      </video>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
   <img id="teaser" src="./static/images/system_diagram.png" alt="System Diagram" width="100%">
  </div>
 </section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
   <h2 class="title is-3">Decomposed Instruction Following Policy</h2>
   <img id="teaser" src="./static/images/susie_rollout.png" alt="Decomposed Instruction Following Policy" height="100%">
  </div>
 </section> -->

 <!-- <section class="video-background">
  <div class="container is-max-desktop has-text-centered">
    <video id="teaser" autoplay muted loop playsinline width="90%">
      <source src="static/videos/decomposed_lc_policy.mp4" type="video/mp4">
    </video>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Method Overview</h2>
    <div class="hero-body-border">
      <video id="decomposed_lc_policy" autoplay muted loop playsinline height="80%">
        <source src="./static/videos/soar_lc_policy.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            Our approach WSRL (Warm Start Reinforcement Learning) instantiates these insights into an extremely simple
            and practical method that enables us to obtain strong fine-tuning results without offline data.  
          </p>
          <p>
            WSRL is an online, off-policy, actor-critic algorithm. It initializes the value function
            and policy with the pre-trained Q-function 𝑄pre𝜃 and policy 𝜋pre
            𝜓 which could come from any offline RL algorithm. It then “simulates” continued training on
            offline data by collecting a small number of warmup transitions with the frozen offline RL policy at the onset
            of online fine-tuning. This allows the Q-values to stabilize through a "warmup phase", and can prevent Q-divergence. 
            WSRL then trains both the value and policy using standard temporal-difference (TD) updates and
            policy gradient. 
          </p>
          <p>
            For fine-tuning, we train on these transitions an high updates-to-data (UTD) ratio to mitigate the effects of catestrophic forgetting. 
            To combat issues such as overestimation in the high UTD regime, we use an
            ensemble of Q functions and layer normalization in both the actor and the critic.
            We hope that WSRL sheds light on the challenges in no-retention fine-tuning, and inspire future research on the important paradigm of no-retention RL fine-tuning. 
          </p>
        </div>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <p>
      WSRL is able to recover from the initial performance dip during finetuning, and is able to finetune efficiently compared to various no-retention RL algorithms across various environments. 
    </p>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0;">
      <img id="teaser" src="../images/paper-images/nrf_finetune_results.png" alt="Nrf comparison" width="100%">
    </div>
    <p>
      Even compared to methods that <b>retain offline data</b>, WSRL is able to outperform faster or competitively <b>without</b> retaining offline training data.
    </p>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0;">
      <img id="teaser" src="../images/paper-images/baselines_compare.png" alt="Nrf comparison" width="100%">
    </div>
  </div>
</section>

<section class="hero" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>@article{zhou2024autonomous,
        title={Autonomous Improvement of Instruction Following Skills via Foundation Models},
        author={Zhiyuan Zhou and Pranav Atreya and Abraham Lee and Homer Walke and Oier Mees and Sergey Levine},
        journal = {arXiv preprint arXiv:407.20635},
        year={2024},
        }
      </code>
    </pre>
  </div>
  </section>
<br><br>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>