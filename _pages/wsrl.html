---
permalink: /wsrl/
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="WSRL: Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data">
  <meta name="keywords" content="Principaled Reward Design, Fast Learning, Reinfrocement Learning, Pareto Optimality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>WSRL</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../assets/css/styles.css">
  <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
  <link rel="icon" href="https://example.com/path/to/favicon.ico">
</head>


<body>

<section class="hero">
  <div class="hero-body"  style="padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Efficient Online Reinforcement Learning
            Fine-Tuning Need Not Retain Offline Data</h1>
          <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhouzypaul.github.io/">Zhiyuan Zhou</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href=""></a>Andy Peng<sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://colinqiyangli.github.io">Qiyang Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://aviralkumar2907.github.io">Aviral Kumar</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup><font size="-0.1">1</sup>UC Berkeley</font></span>
            <span class="author-block"><sup><font size="-0.1">2</sup>Carnegie Mellon University </font></span>
            <span class="author-block"><sup><font size="-0.1">*</sup>Equal Contribution </font></span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2412.07762"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/zhouzypaul/wsrl"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/rail-berkeley/soar?tab=readme-ov-file#using-soar-data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="
                      far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="video-background">
  <div class="container is-max-desktop has-text-centered">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline width="90%">
        <source src="static/videos/teaser_video.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section> -->

<section class="hero teaser" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="../images/paper-images/wsrl/wsrl_teaser.pdf" alt="System Diagram" width="100%">
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <div class="custom-box">
      <h2 class="title is-2"> <mark>TL;DR</mark></h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              We investigate the role of offline data retention during online RL fine-tuning and the sudden value-function divergence that arises due to the distribution mismatch 
              between the offline dataset and online rollouts. This divergence typically results in <b>unlearning</b> and forgetting the benefits of offline pre-training. To address this problem, we introduce Warm-Start RL (<b>WSRL</b>), 
              an offline-to-online RL algorithm that performs <b>efficient offline-to-online fine-tuning without retaining offline data</b>.
              By initializing the value function and policy with a pretrained Q-function and policy and using warmup steps to collect online rollouts with the frozen offline 
              policy, WSRL is able to simulate the retention of offline data and <b>mitigate the challenges of distribution shift and catestrophic forgetting that comes with 
                offline-to-online RL finetuning.</b
            </p>
          </div>
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Motivation</h2>
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            The modern paradigm in machine learning involves <b>pre-training</b> on diverse data, followed by task-specific <b>fine-tuning</b>. In reinforcement learning (RL), 
            this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. 
            Such a setting is important for truly scalable RL.
    
          </p>
          <p>
            We find that <b>without</b> retaining offline data, current offline-to-online RL algorithms fail in this no-retention setting because of Q-value divergence,
            and forget knowledge learned from pre-training. 

            Unsurprisingly, when continually training <b>with</b> offline data during fine-tuning, these algorithms work as intended. But continued training on offline data for stability
             and performance can be slow, expensive, and limiting due to constraints or pessimism on the offline data. 
            Is it possible to fine-tune from offline RL value and policy initializations, but <b>without retaining offline data</b> or forgetting the pre-training? 
          </p>
          <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto;">
            <img src="../images/paper-images/wsrl/challenges.png" alt="">
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Challenges</h2>
    <div class="columns">
      <div class="column">

        <div class="custom-box" style="padding-bottom: 20px; margin-bottom: 20px;">
          <div class="content has-text-justified">
            <h3> 
              Why do previous methods need to retain offline data?
            </h3>
            <p>
              Training <b>only on online experience</b> during fine-tuning without offline data retention can <b>destroy</b> how well the model fits offline data: 
              despite attaining comparable TD-errors on the online data to the setting when offline data is retained, 
              TD-errors under the offline distribution grow larger.
            </p>
            <p>
              The distribution shift between offline dataset data and online rollout data distributions causes a <b>divergence of Q-values</b> at the onset of finetuning.
              We find that Q-value recalibration at the beginning of fine-tuning leads to excessive underestimation due to backups with over-pessimistic TD-targets 
              and destroys Q-function fit.
              We call this the <b>Downward Spiral</b>, and is shown across 3 kitchen envs with 3 offline RL methods
            </p>
          </div>
        </div>

        <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
          <img src="../images/paper-images/wsrl/calql_different_mixing.png" style="width: 25%;" alt="Mixing Image">
          <img src="../images/paper-images/wsrl/calql_different_mixing_q_blowup.png" style="width: 25%;" alt="Q values Blowup">
          <img src="../images/paper-images/wsrl/calql_different_mixing_loss_blowup.png" style="width: 25%;" alt="TD Error Blowup">
          <img src="../images/paper-images/wsrl/calql_different_mixing_loss_online.png" style="width: 25%;" alt="TD Loss Online">
        </div>

        <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
          <img src="../images/paper-images/wsrl/downwards_spiral_cql.png" style="width: 33%;" alt="Downward Spiral CQL">
          <img src="../images/paper-images/wsrl/downward-spiral-calql.png" style="width: 33%;" alt="Downward Spiral CalQL">
          <img src="../images/paper-images/wsrl/downward-spiral-iql.png" style="width: 33%;" alt="Downward Spiral IQL">
        </div>

        <div class="content has-text-justified">
          <div class="custom-box" style="padding-bottom: 20px; margin-bottom: 20px;">
            <h3> 
              Retaining offline data hurts asymptotic performance
            </h3>
            <p>
              While retaining offline data appears to be crucial for preventing forgetting at the beginning of fine-tuning for current fine-tuning methods,
               continuing to make updates on this offline data with an (pesimistic) offline RL algorithm negatively impacts asymptotic performance and efficiency.
              We show this by comparing RLPD (which starts from scratch), against CalQL with data retention.
            </p>
          </div>
          <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
            <img id="teaser" src="../images/paper-images/wsrl/calql_rlpd_compare.png" alt="System Diagram" width="100%">
          </div>


    </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
      <video id="teaser" autoplay muted playsinline height="100%">
        <source src="./static/videos/animated_diagram.mp4"
                type="video/mp4">
      </video>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
   <img id="teaser" src="./static/images/system_diagram.png" alt="System Diagram" width="100%">
  </div>
 </section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
   <h2 class="title is-3">Decomposed Instruction Following Policy</h2>
   <img id="teaser" src="./static/images/susie_rollout.png" alt="Decomposed Instruction Following Policy" height="100%">
  </div>
 </section> -->

 <!-- <section class="video-background">
  <div class="container is-max-desktop has-text-centered">
    <video id="teaser" autoplay muted loop playsinline width="90%">
      <source src="static/videos/decomposed_lc_policy.mp4" type="video/mp4">
    </video>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Method Overview</h2>
    <div class="hero-body-border">
      <video id="decomposed_lc_policy" autoplay muted loop playsinline height="80%">
        <source src="./static/videos/soar_lc_policy.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            Our approach WSRL (Warm-Start Reinforcement Learning) instantiates these insights into an extremely simple
            and practical method that enables us to obtain strong fine-tuning results and quickly adapt online without using the offline dataset during online updates,
             mirroring the common paradigm in machine learning at scale today.  
          </p>
          <p>
            WSRL is an online, off-policy, actor-critic algorithm. It initializes the value function
            and policy with the pre-trained Q-function 𝑄pre𝜃 and policy 𝜋pre
            𝜓 which could come from any offline RL algorithm. It then “simulates” continued training on
            offline data by collecting a small number of warmup transitions with the frozen offline RL policy at the onset
            of online fine-tuning. This allows the Q-values to stabilize through a "warmup phase", and can prevent Q-divergence and minimize catestrophic
             forgetting.  
            WSRL then trains both the value and policy using standard temporal-difference (TD) updates and
            policy gradient. 
          </p>
          <p>
            We hope that WSRL sheds light on the challenges in no-retention fine-tuning, and inspires future research on the important paradigm of no-retention RL fine-tuning. 
          </p>
        </div>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <p>
      WSRL is able to recover from the initial performance dip during finetuning, and is able to finetune efficiently compared to various <b>no-retention</b> RL algorithms across various environments. 
    </p>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0;  padding-bottom: 40px;">
      <img id="teaser" src="../images/paper-images/wsrl/nrf_finetune_results.png" alt="Nrf comparison" width="100%">
    </div>
    <p>
      Even compared to methods that <b>retain offline data</b>, WSRL is able to outperform faster or competitively <b>without</b> retaining offline training data.
    </p>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0;">
      <img id="teaser" src="../images/paper-images/wsrl/baselines_compare.png" alt="Nrf comparison" width="100%">
    </div>
  </div>
</section>

<section class="hero" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre>
      <code>@article{zhou2024efficient,
        author = {Zhiyuan Zhou and Andy Peng and Qiyang Li and Sergey Levine and Aviral Kumar},
        title  = {Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data},
        conference = {arXiv Pre-print},
        year = {2024},
        url = {http://arxiv.org/abs/2412.07762},
      }
      </code>
    </pre>
  </div>
  </section>
<br><br>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>